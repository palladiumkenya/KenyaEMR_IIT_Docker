{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7108e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadr\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_curve\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from datetime import timedelta\n",
    "import random\n",
    "import os\n",
    "import boto3\n",
    "import tempfile\n",
    "from settings import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc6443c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AWS_ACCESS_KEY_ID'] = settings.AWS_ACCESS_KEY_ID\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = settings.AWS_SECRET_ACCESS_KEY\n",
    "os.environ['AWS_DEFAULT_REGION'] = settings.AWS_DEFAULT_REGION\n",
    "\n",
    "\n",
    "# Define S3 info\n",
    "bucket_name = 'kehmisjan2025'\n",
    "file_key = 'targets_apr23.rds'\n",
    "\n",
    "# Initialize boto3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Download to a temporary file\n",
    "with tempfile.NamedTemporaryFile(suffix=\".rds\") as tmp_file:\n",
    "    s3.download_fileobj(bucket_name, file_key, tmp_file)\n",
    "    tmp_file.seek(0)  # go back to beginning\n",
    "    result = pyreadr.read_r(tmp_file.name)  # returns a dictionary\n",
    "\n",
    "# Extract the data frame\n",
    "iit_data = next(iter(result.values()))  # assumes only one object inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "848658d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4292/2733080022.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iit_data['is_friday'] = iit_data['Day'].apply(lambda x: 1 if x == \"Fri\" else 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-04 2024-09-28\n",
      "2022-01-01 00:00:00 2024-09-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "iit_data['NAD'] = pd.to_datetime(iit_data['NAD'], format='%Y-%m-%d')\n",
    "start_exclude = pd.Timestamp('2024-10-01')\n",
    "end_exclude = pd.Timestamp('2024-12-31')\n",
    "\n",
    "# Filter out records from Sept through Dec 2024\n",
    "iit_data = iit_data[~((iit_data['NAD'] >= start_exclude) & (iit_data['NAD'] <= end_exclude))]\n",
    "iit_data['is_friday'] = iit_data['Day'].apply(lambda x: 1 if x == \"Fri\" else 0)\n",
    "print(iit_data['VisitDate'].min(),iit_data['VisitDate'].max())\n",
    "print(iit_data['NAD'].min(),iit_data['NAD'].max())\n",
    "\n",
    "iit_data = iit_data.drop(columns=[\n",
    "    'OptimizedHIVRegimen', 'Drug', 'VisitDate', 'WHO_Missing', 'Type',\n",
    "    'most_recent_cd4', 'regimen_switch', 'AHD', 'NAD_Imputation_Flag',\n",
    "    'BMI_Missing', 'TimeatFacility', 'Adherence_Missing', 'Facility_type_category',\n",
    "    'Pregnant_Missing', 'Breastfeeding_Missing', 'Month', 'Day'\n",
    "    # 'lastvd' to 'months_since_restart' would go here\n",
    "    # 'Month', 'Day' handled below\n",
    "    \n",
    "])\n",
    "selected_columns= ['num_late_last3', 'num_late14_last3', 'num_late30_last3',\n",
    "       'num_late_last5', 'num_late14_last5', 'num_late30_last5',\n",
    "       'num_late_last10', 'num_late14_last10', 'num_late30_last10']\n",
    "iit_data[selected_columns] = iit_data[selected_columns].apply(pd.to_numeric, errors='coerce')\n",
    "# Pregnant: Yes -> 1, No -> 0, else NA\n",
    "iit_data['Pregnant'] = iit_data['Pregnant'].map({'Yes': 1, 'No': 0}).astype('Int64')\n",
    "\n",
    "# Breastfeeding: Yes -> 1, No -> 0, else NA\n",
    "iit_data['Breastfeeding'] = iit_data['Breastfeeding'].map({'Yes': 1, 'No': 0}).astype('Int64')\n",
    "\n",
    "# ARTAdherence: good -> 1, poor/fair -> 0, else NA\n",
    "iit_data['ARTAdherence'] = iit_data['ARTAdherence'].map({\n",
    "    'good': 1,\n",
    "    'poor': 0,\n",
    "    'fair': 0\n",
    "}).astype('Int64')\n",
    "\n",
    "# Sex: Male -> 1, else 0\n",
    "iit_data['Sex'] = (iit_data['Sex'] == 'Male').astype('Int64')\n",
    "\n",
    "# Emr: KenyaEMR -> 1, else 0\n",
    "iit_data['Emr'] = (iit_data['Emr'] == 'KenyaEMR').astype('Int64')  # assuming there is an 'Emr' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83e1f93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_xgboost(dataset):\n",
    "    # List of categorical variables to be encoded\n",
    "    categorical_columns = [ 'BMI', 'WHOStage','most_recent_vl', 'MaritalStatus', 'EducationLevel','DifferentiatedCare',\n",
    "       'Occupation', 'VisitBy','TCAReason', 'cascade_status', 'Kephlevel','Ownertype'] \n",
    "    \n",
    "    # One-hot encoding the categorical columns\n",
    "    ohe = pd.get_dummies(dataset[categorical_columns], drop_first=True, dtype=int)\n",
    "    \n",
    "    # Concatenate the original dataset (excluding categorical columns) with the one-hot encoded columns\n",
    "    dataset_encoded = pd.concat([dataset.drop(columns=categorical_columns), ohe], axis=1)\n",
    "    \n",
    "    return dataset_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6164e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode_xgboost(dataset):\n",
    "#     # List of categorical variables to be encoded\n",
    "#     categorical_columns = ['most_recent_vl','DifferentiatedCare',\n",
    "#         'VisitBy','cascade_status', 'Kephlevel','Ownertype'] \n",
    "    \n",
    "#     # One-hot encoding the categorical columns\n",
    "#     ohe = pd.get_dummies(dataset[categorical_columns], drop_first=True, dtype=int)\n",
    "    \n",
    "#     # Concatenate the original dataset (excluding categorical columns) with the one-hot encoded columns\n",
    "#     dataset_encoded = pd.concat([dataset.drop(columns=categorical_columns), ohe], axis=1)\n",
    "    \n",
    "#     return dataset_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23ca45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iit_data= iit_data.drop([ 'BMI', 'WHOStage','MaritalStatus', 'EducationLevel','Occupation','TCAReason', 'num_late30_last3',\n",
    "#        'lateness_last5', 'num_late_last5', 'num_late14_last5',\n",
    "#        'num_late30_last5', 'lateness_last10', 'num_late_last10',\n",
    "#        'num_late14_last10', 'num_late30_last10','men_knowledge', 'women_knowledge',\n",
    "#        'men_heardaids', 'men_highrisksex', 'men_highrisksex_multi',\n",
    "#        'men_sexnotwithpartner', 'men_sexpartners', 'men_nevertested',\n",
    "#        'men_testedrecent', 'men_sti', 'women_heardaids', 'women_highrisksex',\n",
    "#        'women_highrisksex_multi', 'women_sexnotwithpartner',\n",
    "#        'women_sexpartners', 'women_nevertested', 'women_testedrecent'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57a675b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Temporal Split Function\n",
    "# -----------------------------\n",
    "def preprocess_split(df, start_date, end_date):\n",
    "    split = df[(df[\"NAD\"] >= start_date) & (df[\"NAD\"] <= end_date)].copy()\n",
    "    # Drop columns if they exist\n",
    "    for col in [\"SiteCode\", \"key\", \"NAD\"]:\n",
    "        if col in split.columns:\n",
    "            split = split.drop(columns=col)\n",
    "    X = split.drop(columns=[\"iit\"])\n",
    "    y = split[\"iit\"]\n",
    "    return X, y\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Apply Temporal Splits\n",
    "# -----------------------------\n",
    "X_train_df, y_train = preprocess_split(iit_data, \"2024-01-01\", \"2024-05-31\")\n",
    "X_val_df, y_val = preprocess_split(iit_data, \"2024-06-01\", \"2024-06-30\")\n",
    "X_testnear_df, y_testnear = preprocess_split(iit_data, \"2024-07-01\", \"2024-07-30\")\n",
    "X_test_df, y_test = preprocess_split(iit_data, \"2024-07-01\", \"2024-09-30\")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Impute Missing Values\n",
    "# -----------------------------\n",
    "numeric_cols = X_train_df.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = X_train_df.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "imputer = ColumnTransformer([\n",
    "    ('num', SimpleImputer(strategy='mean'), numeric_cols),\n",
    "    ('cat', SimpleImputer(strategy='most_frequent'), categorical_cols)\n",
    "])\n",
    "\n",
    "X_train_imputed = imputer.fit_transform(X_train_df)\n",
    "X_val_imputed = imputer.transform(X_val_df)\n",
    "X_testnear_imputed = imputer.transform(X_testnear_df)\n",
    "X_test_imputed = imputer.transform(X_test_df)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Rebuild DataFrames\n",
    "# -----------------------------\n",
    "def rebuild_df(data, num_cols, cat_cols):\n",
    "    return pd.DataFrame(data, columns=num_cols + cat_cols)\n",
    "\n",
    "X_train_df = rebuild_df(X_train_imputed, numeric_cols, categorical_cols)\n",
    "X_val_df = rebuild_df(X_val_imputed, numeric_cols, categorical_cols)\n",
    "X_testnear_df = rebuild_df(X_testnear_imputed, numeric_cols, categorical_cols)\n",
    "X_test_df = rebuild_df(X_test_imputed, numeric_cols, categorical_cols)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 5: One-Hot Encode \n",
    "# -----------------------------\n",
    "X_train_encoded = encode_xgboost(X_train_df)\n",
    "X_val_encoded = encode_xgboost(X_val_df)\n",
    "X_testnear_encoded = encode_xgboost(X_testnear_df)\n",
    "X_test_encoded = encode_xgboost(X_test_df)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 6: Align One-Hot Encoded Columns\n",
    "# -----------------------------\n",
    "all_cols = X_train_encoded.columns\n",
    "X_val_encoded = X_val_encoded.reindex(columns=all_cols, fill_value=0)\n",
    "X_testnear_encoded = X_testnear_encoded.reindex(columns=all_cols, fill_value=0)\n",
    "X_test_encoded = X_test_encoded.reindex(columns=all_cols, fill_value=0)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 7: Scale Numeric Columns\n",
    "# -----------------------------\n",
    "# Identify numeric columns from the encoded DataFrame that were originally numeric\n",
    "final_numeric_cols = [col for col in numeric_cols if col in X_train_encoded.columns]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_encoded[final_numeric_cols] = scaler.fit_transform(X_train_encoded[final_numeric_cols])\n",
    "X_val_encoded[final_numeric_cols] = scaler.transform(X_val_encoded[final_numeric_cols])\n",
    "X_testnear_encoded[final_numeric_cols] = scaler.transform(X_testnear_encoded[final_numeric_cols])\n",
    "X_test_encoded[final_numeric_cols] = scaler.transform(X_test_encoded[final_numeric_cols])\n",
    "\n",
    "# -----------------------------\n",
    "# Step 8: Convert to PyTorch Tensors\n",
    "# -----------------------------\n",
    "X_train = torch.tensor(X_train_encoded.values, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "\n",
    "X_val = torch.tensor(X_val_encoded.values, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "\n",
    "X_testnear = torch.tensor(X_testnear_encoded.values, dtype=torch.float32)\n",
    "y_testnear = torch.tensor(y_testnear.values, dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor(X_test_encoded.values, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2d6763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# Model definition\n",
    "class TabularTransformer(nn.Module):\n",
    "    def __init__(self, num_features, dim=4, heads=1, depth=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(num_features, dim) #Projects the original tabular input into a dense vector space of size dim.\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim, nhead=heads, dropout=dropout, batch_first=True\n",
    "        ) #Transformer encoder layers\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(dim, 16), # linear embedding layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)  # Add sequence dimension for transformer\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "        return self.classifier(x).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deb4534d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1229\n",
      "Epoch 2, Loss: 0.1189\n",
      "Epoch 3, Loss: 0.1185\n",
      "Epoch 4, Loss: 0.1184\n",
      "Epoch 5, Loss: 0.1183\n",
      "Epoch 6, Loss: 0.1182\n",
      "Epoch 7, Loss: 0.1181\n",
      "Epoch 8, Loss: 0.1181\n",
      "Epoch 9, Loss: 0.1180\n",
      "Epoch 10, Loss: 0.1180\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Instantiate model\n",
    "model = TabularTransformer(num_features=X_train.shape[1])\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Modify training loop\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(batch_X)\n",
    "        loss = criterion(y_pred, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee43f23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test near ROC-AUC: 0.77\n",
      "test near AUC-PR: 0.13\n",
      "Test ROC-AUC: 0.75\n",
      "Test AUC-PR: 0.12\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function \n",
    "def evaluate(model, X, y, label=\"\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(X)\n",
    "        preds_np = preds.numpy()\n",
    "        y_np = y.numpy()\n",
    "\n",
    "        roc_auc = roc_auc_score(y_np, preds_np)\n",
    "        auc_pr = average_precision_score(y_np, preds_np)\n",
    "\n",
    "        print(f\"{label} ROC-AUC: {roc_auc:.2f}\")\n",
    "        print(f\"{label} AUC-PR: {auc_pr:.2f}\")\n",
    "\n",
    "# Evaluate on validation set (June)\n",
    "evaluate(model, X_testnear, y_testnear, label=\"test near\")\n",
    "\n",
    "# Evaluate on test set (Julyâ€“Sept)\n",
    "evaluate(model, X_test, y_test, label=\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70bf943",
   "metadata": {},
   "source": [
    "Reshaping the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b2467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from collections import defaultdict\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# # -------------------------------\n",
    "# # Step 1: Preprocessing - Impute, Encode, and Scale\n",
    "# # -------------------------------\n",
    "\n",
    "# def preprocess_features(df, target_col=\"iit\"):\n",
    "#     features = df.drop(columns=[target_col, \"SiteCode\", \"NAD\", \"key\"], errors=\"ignore\")\n",
    "#     target = df[target_col]\n",
    "\n",
    "#     numeric_cols = features.select_dtypes(include=np.number).columns.tolist()\n",
    "#     categorical_cols = features.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "#     # Impute\n",
    "#     imputer = ColumnTransformer([\n",
    "#         ('num', SimpleImputer(strategy='mean'), numeric_cols),\n",
    "#         ('cat', SimpleImputer(strategy='most_frequent'), categorical_cols)\n",
    "#     ])\n",
    "#     imputed = imputer.fit_transform(features)\n",
    "#     imputed_df = pd.DataFrame(imputed, columns=numeric_cols + categorical_cols)\n",
    "\n",
    "#     # One-hot encode\n",
    "#     encoded_df = pd.get_dummies(imputed_df, columns=categorical_cols)\n",
    "\n",
    "#     # Scale numeric\n",
    "#     scaler = StandardScaler()\n",
    "#     encoded_df[numeric_cols] = scaler.fit_transform(encoded_df[numeric_cols])\n",
    "\n",
    "#     return encoded_df, target, numeric_cols, encoded_df.columns\n",
    "\n",
    "# # -------------------------------\n",
    "# # Step 2: Convert to Sequences\n",
    "# # -------------------------------\n",
    "# #Builds sequences of visits per patient (key), sorted by date (NAD).\n",
    "# # Extracts features as a sequence\n",
    "\n",
    "\n",
    "\n",
    "# def create_sequences(df, feature_cols, label_col=\"iit\", max_len=None): \n",
    "#     df = df.sort_values([\"key\", \"NAD\"])\n",
    "#     grouped = df.groupby(\"key\")\n",
    "\n",
    "#     sequences = []\n",
    "#     labels = []\n",
    "\n",
    "#     for key, group in grouped:\n",
    "#         feats = group[feature_cols].values\n",
    "#         label = group[label_col].values[-1]  \n",
    "#         if max_len:\n",
    "#             if len(feats) < max_len:\n",
    "#                 pad_len = max_len - len(feats)\n",
    "#                 padding = np.zeros((pad_len, feats.shape[1]))\n",
    "#                 feats = np.vstack([padding, feats])\n",
    "#             else:\n",
    "#                 feats = feats[-max_len:]\n",
    "#         sequences.append(feats)\n",
    "#         labels.append(label)\n",
    "\n",
    "#     return torch.tensor(sequences, dtype=torch.float32), torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "# # -------------------------------\n",
    "# # Step 3: Transformer Model for Sequences\n",
    "# # -------------------------------\n",
    "\n",
    "# class VisitTransformer(nn.Module):\n",
    "#     def __init__(self, input_dim, model_dim=32, heads=2, depth=2, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.embedding = nn.Linear(input_dim, model_dim)\n",
    "#         encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=heads, dropout=dropout, batch_first=True)\n",
    "#         self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(model_dim, 16),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(dropout),\n",
    "#             nn.Linear(16, 1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.embedding(x)  # shape: [batch, seq_len, model_dim]\n",
    "#         x = self.transformer(x)  # same shape\n",
    "#         x = x.mean(dim=1)  # average over sequence\n",
    "#         return self.classifier(x).squeeze()\n",
    "\n",
    "# # -------------------------------\n",
    "# # Step 4: Dataset Wrapper\n",
    "# # -------------------------------\n",
    "\n",
    "# class SequenceDataset(Dataset):\n",
    "#     def __init__(self, X_seq, y):\n",
    "#         self.X = X_seq\n",
    "#         self.y = y\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.X)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.X[idx], self.y[idx]\n",
    "\n",
    "# # -------------------------------\n",
    "# # Step 5: Training the Model\n",
    "# # -------------------------------\n",
    "\n",
    "# def train_model(model, dataloader, val_loader, epochs=10):\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "#     criterion = nn.BCELoss()\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         total_loss = 0\n",
    "#         for X_batch, y_batch in dataloader:\n",
    "#             optimizer.zero_grad()\n",
    "#             preds = model(X_batch)\n",
    "#             loss = criterion(preds, y_batch)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#         print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "#         evaluate(model, val_loader)\n",
    "\n",
    "# # -------------------------------\n",
    "# # Step 6: Evaluation\n",
    "# # -------------------------------\n",
    "\n",
    "# def evaluate(model, loader):\n",
    "#     model.eval()\n",
    "#     all_preds, all_labels = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for X, y in loader:\n",
    "#             preds = model(X)\n",
    "#             all_preds.extend(preds.cpu().numpy())\n",
    "#             all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "#     roc = roc_auc_score(all_labels, all_preds)\n",
    "#     pr = average_precision_score(all_labels, all_preds)\n",
    "#     print(f\"ROC-AUC: {roc:.3f}, AUC-PR: {pr:.3f}\")\n",
    "# # -------------------------------\n",
    "# # Step 7: Execution\n",
    "# # -------------------------------\n",
    "\n",
    "# # Preprocess full dataset\n",
    "# full_features, full_target, num_cols, feat_cols = preprocess_features(iit_data)\n",
    "\n",
    "# # Append back identifiers\n",
    "# iit_data_proc = pd.concat([iit_data[[\"key\", \"NAD\", \"iit\"]].reset_index(drop=True), full_features.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# # Split into train/val/test\n",
    "# train_df = iit_data_proc[(iit_data_proc[\"NAD\"] >= \"2024-01-01\") & (iit_data_proc[\"NAD\"] <= \"2024-05-31\")]\n",
    "# val_df = iit_data_proc[(iit_data_proc[\"NAD\"] >= \"2024-06-01\") & (iit_data_proc[\"NAD\"] <= \"2024-06-30\")]\n",
    "# test_df = iit_data_proc[(iit_data_proc[\"NAD\"] >= \"2024-07-01\") & (iit_data_proc[\"NAD\"] <= \"2024-09-30\")]\n",
    "\n",
    "# # Define max sequence length\n",
    "# MAX_SEQ_LEN = 10  # Adjust based on your data distribution\n",
    "\n",
    "# # Create sequences\n",
    "# X_train_seq, y_train_seq = create_sequences(train_df, feat_cols, max_len=MAX_SEQ_LEN)\n",
    "# X_val_seq, y_val_seq = create_sequences(val_df, feat_cols, max_len=MAX_SEQ_LEN)\n",
    "# X_test_seq, y_test_seq = create_sequences(test_df, feat_cols, max_len=MAX_SEQ_LEN)\n",
    "\n",
    "# # DataLoaders\n",
    "# train_loader = DataLoader(SequenceDataset(X_train_seq, y_train_seq), batch_size=64, shuffle=True)\n",
    "# val_loader = DataLoader(SequenceDataset(X_val_seq, y_val_seq), batch_size=64)\n",
    "# test_loader = DataLoader(SequenceDataset(X_test_seq, y_test_seq), batch_size=64)\n",
    "\n",
    "# # Initialize and train model\n",
    "# model = VisitTransformer(input_dim=X_train_seq.shape[2])\n",
    "# train_model(model, train_loader, val_loader, epochs=10)\n",
    "\n",
    "# # Final evaluation\n",
    "# evaluate(model, test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7edae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -------------------------------\n",
    "# # Step 7: Execution\n",
    "# # -------------------------------\n",
    "\n",
    "# # Preprocess full dataset\n",
    "# full_features, full_target, num_cols, feat_cols = preprocess_features(iit_data)\n",
    "\n",
    "# # Append back identifiers\n",
    "# iit_data_proc = pd.concat([iit_data[[\"key\", \"NAD\", \"iit\"]].reset_index(drop=True), full_features.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# # Split into train/val/test\n",
    "# train_df = iit_data_proc[(iit_data_proc[\"NAD\"] >= \"2024-01-01\") & (iit_data_proc[\"NAD\"] <= \"2024-05-31\")]\n",
    "# val_df = iit_data_proc[(iit_data_proc[\"NAD\"] >= \"2024-06-01\") & (iit_data_proc[\"NAD\"] <= \"2024-06-30\")]\n",
    "# test_df = iit_data_proc[(iit_data_proc[\"NAD\"] >= \"2024-07-01\") & (iit_data_proc[\"NAD\"] <= \"2024-09-30\")]\n",
    "\n",
    "# # Define max sequence length\n",
    "# MAX_SEQ_LEN = 10  # Adjust based on your data distribution\n",
    "\n",
    "# # Create sequences\n",
    "# X_train_seq, y_train_seq = create_sequences(train_df, feat_cols, max_len=MAX_SEQ_LEN)\n",
    "# X_val_seq, y_val_seq = create_sequences(val_df, feat_cols, max_len=MAX_SEQ_LEN)\n",
    "# X_test_seq, y_test_seq = create_sequences(test_df, feat_cols, max_len=MAX_SEQ_LEN)\n",
    "\n",
    "# # DataLoaders\n",
    "# train_loader = DataLoader(SequenceDataset(X_train_seq, y_train_seq), batch_size=64, shuffle=True)\n",
    "# val_loader = DataLoader(SequenceDataset(X_val_seq, y_val_seq), batch_size=64)\n",
    "# test_loader = DataLoader(SequenceDataset(X_test_seq, y_test_seq), batch_size=64)\n",
    "\n",
    "# # Initialize and train model\n",
    "# model = VisitTransformer(input_dim=X_train_seq.shape[2])\n",
    "# train_model(model, train_loader, val_loader, epochs=10)\n",
    "\n",
    "# # Final evaluation\n",
    "# evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c1b712f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/tmp/ipykernel_4292/1379678901.py:59: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  return torch.tensor(sequences, dtype=torch.float32), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1244\n",
      "ROC-AUC: 0.750, AUC-PR: 0.099\n",
      "Epoch 2, Loss: 0.1223\n",
      "ROC-AUC: 0.762, AUC-PR: 0.117\n",
      "Epoch 3, Loss: 0.1216\n",
      "ROC-AUC: 0.748, AUC-PR: 0.113\n",
      "Epoch 4, Loss: 0.1214\n",
      "ROC-AUC: 0.750, AUC-PR: 0.104\n",
      "Epoch 5, Loss: 0.1212\n",
      "ROC-AUC: 0.752, AUC-PR: 0.120\n",
      "Epoch 6, Loss: 0.1208\n",
      "ROC-AUC: 0.754, AUC-PR: 0.126\n",
      "Epoch 7, Loss: 0.1206\n",
      "ROC-AUC: 0.752, AUC-PR: 0.122\n",
      "Epoch 8, Loss: 0.1204\n",
      "ROC-AUC: 0.757, AUC-PR: 0.127\n",
      "Epoch 9, Loss: 0.1203\n",
      "ROC-AUC: 0.757, AUC-PR: 0.129\n",
      "Epoch 10, Loss: 0.1202\n",
      "ROC-AUC: 0.748, AUC-PR: 0.122\n",
      "\n",
      "Final Evaluation on Test Set:\n",
      "ROC-AUC: 0.726, AUC-PR: 0.091\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from datetime import datetime\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Preprocessing\n",
    "# -------------------------------\n",
    "def preprocess_features(df, target_col=\"iit\"):\n",
    "    features = df.drop(columns=[target_col, \"SiteCode\", \"NAD\", \"key\"], errors=\"ignore\")\n",
    "    target = df[target_col]\n",
    "\n",
    "    numeric_cols = features.select_dtypes(include=np.number).columns.tolist()\n",
    "    categorical_cols = features.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "    imputer = ColumnTransformer([\n",
    "        ('num', SimpleImputer(strategy='mean'), numeric_cols),\n",
    "        ('cat', SimpleImputer(strategy='most_frequent'), categorical_cols)\n",
    "    ])\n",
    "    imputed = imputer.fit_transform(features)\n",
    "    imputed_df = pd.DataFrame(imputed, columns=numeric_cols + categorical_cols)\n",
    "\n",
    "    encoded_df = pd.get_dummies(imputed_df, columns=categorical_cols)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    encoded_df[numeric_cols] = scaler.fit_transform(encoded_df[numeric_cols])\n",
    "\n",
    "    return encoded_df, target, numeric_cols, encoded_df.columns\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Build Sequences\n",
    "# -------------------------------\n",
    "def create_sequences(df, feature_cols, label_col=\"label_for_eval\", max_len=10):\n",
    "    sequences, labels = [], []\n",
    "    for key, group in df.groupby(\"key\"):\n",
    "        group = group.sort_values(\"NAD\")\n",
    "        features = group[feature_cols].values\n",
    "        labels_seq = group[label_col].values\n",
    "\n",
    "        for i in range(1, len(features) + 1):\n",
    "            seq = features[max(0, i - max_len):i]\n",
    "            label = labels_seq[i - 1]\n",
    "\n",
    "            if np.isnan(label):\n",
    "                continue\n",
    "\n",
    "            if len(seq) < max_len:\n",
    "                pad_len = max_len - len(seq)\n",
    "                seq = np.pad(seq, ((pad_len, 0), (0, 0)), mode=\"constant\")\n",
    "            sequences.append(seq)\n",
    "            labels.append(label)\n",
    "\n",
    "    return torch.tensor(sequences, dtype=torch.float32), torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Transformer Model\n",
    "# -------------------------------\n",
    "class VisitTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim=32, heads=2, depth=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=heads, dropout=dropout, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(model_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.classifier(x).squeeze()\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Dataset Wrapper\n",
    "# -------------------------------\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, X_seq, y):\n",
    "        self.X = X_seq\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# -------------------------------\n",
    "# Step 5: Training & Evaluation\n",
    "# -------------------------------\n",
    "def train_model(model, dataloader, val_loader, epochs=10):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "        evaluate(model, val_loader)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            preds = model(X)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    roc = roc_auc_score(all_labels, all_preds)\n",
    "    pr = average_precision_score(all_labels, all_preds)\n",
    "    print(f\"ROC-AUC: {roc:.3f}, AUC-PR: {pr:.3f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 6: Helper for History + Labels\n",
    "# -------------------------------\n",
    "def get_split_data(df, label_start, label_end):\n",
    "    label_start = pd.to_datetime(label_start)\n",
    "    label_end = pd.to_datetime(label_end)\n",
    "\n",
    "    target_keys = df[(df[\"NAD\"] >= label_start) & (df[\"NAD\"] <= label_end)][\"key\"].unique()\n",
    "    history = df[(df[\"key\"].isin(target_keys)) & (df[\"NAD\"] <= label_end)].copy()\n",
    "\n",
    "    label_visits = df[(df[\"key\"].isin(target_keys)) & (df[\"NAD\"] >= label_start) & (df[\"NAD\"] <= label_end)][[\"key\", \"NAD\"]]\n",
    "    label_visits[\"label_marker\"] = 1\n",
    "\n",
    "    history = pd.merge(history, label_visits, on=[\"key\", \"NAD\"], how=\"left\")\n",
    "    history[\"label_marker\"] = history[\"label_marker\"].fillna(0)\n",
    "    history[\"label_for_eval\"] = history[\"iit\"].where(history[\"label_marker\"] == 1, np.nan)\n",
    "\n",
    "    return history\n",
    "\n",
    "# -------------------------------\n",
    "# Step 7: Run Everything\n",
    "# -------------------------------\n",
    "# Preprocess\n",
    "full_features, full_target, num_cols, feat_cols = preprocess_features(iit_data)\n",
    "iit_data_proc = pd.concat([iit_data[[\"key\", \"NAD\", \"iit\"]].reset_index(drop=True),\n",
    "                           full_features.reset_index(drop=True)], axis=1)\n",
    "iit_data_proc[\"NAD\"] = pd.to_datetime(iit_data_proc[\"NAD\"])\n",
    "\n",
    "MAX_SEQ_LEN = 10\n",
    "\n",
    "# ---- Train ----\n",
    "train_df = iit_data_proc[\n",
    "    (iit_data_proc[\"NAD\"] >= pd.to_datetime(\"2023-08-01\")) &\n",
    "    (iit_data_proc[\"NAD\"] <= pd.to_datetime(\"2024-05-31\"))\n",
    "].copy()\n",
    "train_df[\"label_for_eval\"] = train_df[\"iit\"]\n",
    "X_train_seq, y_train_seq = create_sequences(train_df, feat_cols, max_len=MAX_SEQ_LEN)\n",
    "\n",
    "# ---- Validation ----\n",
    "val_df = get_split_data(iit_data_proc, \"2024-06-01\", \"2024-06-30\")\n",
    "X_val_seq, y_val_seq = create_sequences(val_df, feat_cols, max_len=MAX_SEQ_LEN)\n",
    "\n",
    "# ---- Test ----\n",
    "test_df = get_split_data(iit_data_proc, \"2024-07-01\", \"2024-09-30\")\n",
    "X_test_seq, y_test_seq = create_sequences(test_df, feat_cols, max_len=MAX_SEQ_LEN)\n",
    "\n",
    "# ---- Loaders ----\n",
    "train_loader = DataLoader(SequenceDataset(X_train_seq, y_train_seq), batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(SequenceDataset(X_val_seq, y_val_seq), batch_size=64)\n",
    "test_loader = DataLoader(SequenceDataset(X_test_seq, y_test_seq), batch_size=64)\n",
    "\n",
    "# ---- Model ----\n",
    "model = VisitTransformer(input_dim=X_train_seq.shape[2])\n",
    "train_model(model, train_loader, val_loader, epochs=10)\n",
    "\n",
    "# ---- Final Eval ----\n",
    "print(\"\\nFinal Evaluation on Test Set:\")\n",
    "evaluate(model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
